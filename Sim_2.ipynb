{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.function import RobustAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0\n",
    "kappa_buy=15\n",
    "kappa_sell=15\n",
    "lambda_buy=2\n",
    "lambda_sell=2\n",
    "sigma=0.05\n",
    "q_upper=3\n",
    "q_lower=-3\n",
    "Time=10\n",
    "dt=0.01\n",
    "theta=0.01\n",
    "phi=0\n",
    "phi_alpha=20\n",
    "phi_lambda=10\n",
    "phi_kappa=1\n",
    "\n",
    "agent = RobustAgent(\n",
    "    alpha=alpha,\n",
    "    kappa_buy=kappa_buy,\n",
    "    kappa_sell=kappa_sell,\n",
    "    lambda_buy=lambda_buy,\n",
    "    lambda_sell=lambda_sell,\n",
    "    sigma=sigma,\n",
    "    q_upper=q_upper,\n",
    "    q_lower=q_lower,\n",
    "    Time=Time,\n",
    "    dt=dt,\n",
    "    theta=theta,\n",
    "    phi=phi,\n",
    "    phi_alpha=phi_alpha,\n",
    "    phi_kappa=phi_kappa,\n",
    "    phi_lambda=phi_lambda,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5)) \n",
    "\n",
    "for q in range(q_upper - q_lower):\n",
    "    ax[0].plot(agent.time_grid, agent.ops_uncertainty_num[0][:, q], color=\"blue\")\n",
    "    ax[0].plot(agent.time_grid, agent.ops_nouncertainty[0][:, q], '--', color=\"blue\")\n",
    "    # ax[0].plot(agent.time_grid, agent.ops_uncetainty_num[0][:, q], '--', color=\"red\")\n",
    "# plt.plot(agent.time_grid, agent.ops_uncertainty[0])\n",
    "# plt.plot(agent.time_grid, agent.optimal_strategy_no_uncertainty[0], '--')\n",
    "ax[0].set_xlabel(\"Time (secs)\")\n",
    "ax[0].set_ylabel(\"Sell Depth ($\\delta^+$)\")\n",
    "\n",
    "\n",
    "for q in range(3):\n",
    "    # q = 0 , 1 ,2 \n",
    "    # index in sell is q_upper - q, index in buy is q_upper - 1 -q\n",
    "    ax[1].plot(agent.time_grid, agent.ops_uncertainty_num[0][:, q_upper-q] + agent.ops_uncertainty_num[1][:, q_upper-1-q], color=\"blue\")\n",
    "    ax[1].plot(agent.time_grid, agent.ops_nouncertainty[0][:, q_upper-q] + agent.ops_nouncertainty[1][:, q_upper-1-q], '--',color=\"blue\")\n",
    "    # ax[1].plot(agent.time_grid, agent.ops_uncetainty_num[0][:, q_upper-q] + agent.ops_uncetainty_num[1][:, q_upper-1-q], '--',color=\"red\")\n",
    "\n",
    "ax[1].set_xlabel(\"Time (secs)\")\n",
    "ax[1].set_ylabel(\"Total Depth ($\\delta^+ + \\delta^-$)\")\n",
    "# ax[1].set_ylim(0.13, 0.16)\n",
    "\n",
    "# fig.suptitle(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the performance between robust control and plain control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the agent's strategy parameters \n",
    "alpha=0\n",
    "kappa_buy=15\n",
    "kappa_sell=15\n",
    "lambda_buy=0.5\n",
    "lambda_sell=0.5\n",
    "sigma=0.1\n",
    "q_upper=15\n",
    "q_lower=-15\n",
    "Time=10\n",
    "dt=0.01\n",
    "theta=0.01\n",
    "phi=0\n",
    "phi_alpha=20\n",
    "phi_lambda=10\n",
    "phi_kappa=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_samples = 20\n",
    "\n",
    "MC_reward_robust = []\n",
    "MC_reward_plain = []\n",
    "# Set the environment parameters (add midprice drift/ different MO coming rate & filled rate)\n",
    "env_alpha = 0.05\n",
    "env_kappa_buy = 5\n",
    "env_kappa_sell = 5\n",
    "env_lambda_buy = 2\n",
    "env_lambda_sell = 2\n",
    "\n",
    "for _ in range(MC_samples):\n",
    "    reward_with_uncertainty = []\n",
    "    reward_without_uncertainty = []\n",
    "\n",
    "    for _ in range(50):\n",
    "        agent_with_uncertainty = RobustAgent(\n",
    "            alpha=alpha,\n",
    "            kappa_buy=kappa_buy,\n",
    "            kappa_sell=kappa_sell,\n",
    "            lambda_buy=lambda_buy,\n",
    "            lambda_sell=lambda_sell,\n",
    "            sigma=sigma,\n",
    "            q_upper=q_upper,\n",
    "            q_lower=q_lower,\n",
    "            Time=Time,\n",
    "            dt=dt,\n",
    "            theta=theta,\n",
    "            phi=phi,\n",
    "            phi_alpha=phi_alpha,\n",
    "            phi_kappa=phi_kappa,\n",
    "            phi_lambda=phi_lambda,\n",
    "        )\n",
    "\n",
    "        agent_without_uncertainty = RobustAgent(\n",
    "            alpha=alpha,\n",
    "            kappa_buy=kappa_buy,\n",
    "            kappa_sell=kappa_sell,\n",
    "            lambda_buy=lambda_buy,\n",
    "            lambda_sell=lambda_sell,\n",
    "            sigma=sigma,\n",
    "            q_upper=q_upper,\n",
    "            q_lower=q_lower,\n",
    "            Time=Time,\n",
    "            dt=dt,\n",
    "            theta=theta,\n",
    "            phi=phi,\n",
    "            phi_alpha=phi_alpha,\n",
    "            phi_kappa=phi_kappa,\n",
    "            phi_lambda=phi_lambda,\n",
    "        )\n",
    "\n",
    "        agent_with_uncertainty.run(control_uncertainty=True, env_alpha=env_alpha, env_lambda_buy=env_lambda_buy, env_lambda_sell=env_lambda_sell, env_kappa_buy=env_kappa_buy, env_kappa_sell=env_kappa_sell)\n",
    "        agent_without_uncertainty.run(control_uncertainty=False, env_alpha=env_alpha, env_lambda_buy=env_lambda_buy, env_lambda_sell=env_lambda_sell, env_kappa_buy=env_kappa_buy, env_kappa_sell=env_kappa_sell)\n",
    "\n",
    "        reward_with_uncertainty.append(agent_with_uncertainty.objective)\n",
    "        reward_without_uncertainty.append(agent_without_uncertainty.objective)\n",
    "\n",
    "    reward_with_uncertainty = np.array(reward_with_uncertainty)\n",
    "    reward_without_uncertainty = np.array(reward_without_uncertainty)\n",
    "\n",
    "    MC_reward_robust.append(np.mean(reward_with_uncertainty, axis=0))\n",
    "    MC_reward_plain.append(np.mean(reward_without_uncertainty, axis=0))\n",
    "\n",
    "MC_reward_robust = np.array(MC_reward_robust)\n",
    "MC_reward_plain = np.array(MC_reward_plain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent_with_uncertainty.time_grid, np.mean(reward_with_uncertainty, axis=0), label=\"Robust Strategy\")\n",
    "plt.plot(agent_without_uncertainty.time_grid, np.mean(reward_without_uncertainty, axis=0), '--',label=\"Plain Strategy\")\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and 95% CI\n",
    "mean_robust = np.mean(MC_reward_robust, axis=0)\n",
    "mean_plain = np.mean(MC_reward_plain, axis=0)\n",
    "\n",
    "ci_robust = 1.96 * np.std(MC_reward_robust, axis=0) / np.sqrt(MC_samples)\n",
    "ci_plain = 1.96 * np.std(MC_reward_plain, axis=0) / np.sqrt(MC_samples)\n",
    "\n",
    "# Create x-axis values\n",
    "ts = agent_with_uncertainty.time_grid\n",
    "\n",
    "# Plot mean and CI\n",
    "plt.plot(ts, mean_robust, label='Robust', color='blue')\n",
    "plt.fill_between(ts, mean_robust - ci_robust, mean_robust + ci_robust, color='blue', alpha=0.2, label='95% CI Robust')\n",
    "\n",
    "# Plot mean and CI\n",
    "plt.plot(ts, mean_plain, '--', color='red', label='Plain')\n",
    "plt.fill_between(ts, mean_plain - ci_plain, mean_plain + ci_plain, color='red', alpha=0.05, label='95% CI Plain')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Mean Reward with 95% Confidence Interval')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
